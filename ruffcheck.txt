ruff check .

warning: The top-level linter settings are deprecated in favour of their counterparts in the `lint` section. Please update the following options in `pyproject.toml`:                                                                                                                                             
  - 'ignore' -> 'lint.ignore'
  - 'select' -> 'lint.select'
UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\cli.py:37:9
   |
35 |     max_items: int = typer.Option(200, "--max-items", "-n"),
36 |     offline: bool = typer.Option(True, "--offline/--live"),
37 |     db: Optional[str] = typer.Option(None, "--db", help="SQLite database path"),
   |         ^^^^^^^^^^^^^
38 |     timezone: str = typer.Option("America/New_York", "--timezone", "-tz"),
39 | ) -> None:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\cli.py:73:9
   |
71 |     max_items: int = typer.Option(200, "--max-items", "-n"),
72 |     offline: bool = typer.Option(True, "--offline/--live"),
73 |     db: Optional[str] = typer.Option(None, "--db", help="SQLite database path"),
   |         ^^^^^^^^^^^^^
74 |     timezone: str = typer.Option("America/New_York", "--timezone", "-tz"),
75 | ) -> None:
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
   --> scrapesuite\cli.py:117:9
    |
115 | @app.command()
116 | def state(
117 |     db: Optional[str] = typer.Option(None, "--db", help="SQLite database path"),
    |         ^^^^^^^^^^^^^
118 | ) -> None:
119 |     """Show job state (last cursor and last run time)."""
    |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\connectors\base.py:20:23
   |
19 |     def collect(
20 |         self, cursor: Optional[str], max_items: int, offline: bool = True
   |                       ^^^^^^^^^^^^^
21 |     ) -> tuple[list[Raw], Optional[str]]:
22 |         """
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\connectors\base.py:21:27
   |
19 |     def collect(
20 |         self, cursor: Optional[str], max_items: int, offline: bool = True
21 |     ) -> tuple[list[Raw], Optional[str]]:
   |                           ^^^^^^^^^^^^^
22 |         """
23 |         Collect records from source.
   |
help: Convert to `X | None`

F401 [*] `scrapesuite.connectors.base.Connector` imported but unused
 --> scrapesuite\connectors\fda.py:8:41
  |
6 | from bs4 import BeautifulSoup
7 |
8 | from scrapesuite.connectors.base import Connector, Raw
  |                                         ^^^^^^^^^
9 | from scrapesuite.http import get_html
  |
help: Remove unused import: `scrapesuite.connectors.base.Connector`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\connectors\fda.py:16:23
   |
15 |     def collect(
16 |         self, cursor: Optional[str], max_items: int, offline: bool = True
   |                       ^^^^^^^^^^^^^
17 |     ) -> tuple[list[Raw], Optional[str]]:
18 |         """Collect FDA recall records."""
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\connectors\fda.py:17:27
   |
15 |     def collect(
16 |         self, cursor: Optional[str], max_items: int, offline: bool = True
17 |     ) -> tuple[list[Raw], Optional[str]]:
   |                           ^^^^^^^^^^^^^
18 |         """Collect FDA recall records."""
19 |         if offline:
   |
help: Convert to `X | None`

F401 [*] `scrapesuite.connectors.base.Connector` imported but unused
 --> scrapesuite\connectors\nws.py:8:41
  |
6 | from bs4 import BeautifulSoup
7 |
8 | from scrapesuite.connectors.base import Connector, Raw
  |                                         ^^^^^^^^^
9 | from scrapesuite.http import get_html
  |
help: Remove unused import: `scrapesuite.connectors.base.Connector`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\connectors\nws.py:16:23
   |
15 |     def collect(
16 |         self, cursor: Optional[str], max_items: int, offline: bool = True
   |                       ^^^^^^^^^^^^^
17 |     ) -> tuple[list[Raw], Optional[str]]:
18 |         """Collect NWS alert records."""
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\connectors\nws.py:17:27
   |
15 |     def collect(
16 |         self, cursor: Optional[str], max_items: int, offline: bool = True
17 |     ) -> tuple[list[Raw], Optional[str]]:
   |                           ^^^^^^^^^^^^^
18 |         """Collect NWS alert records."""
19 |         if offline:
   |
help: Convert to `X | None`

I001 [*] Import block is un-sorted or un-formatted
  --> scrapesuite\core.py:3:1
   |
 1 |   """Core job loading and execution logic."""
 2 |
 3 | / import os
 4 | | from pathlib import Path
 5 | | from typing import Any
 6 | |
 7 | | import pandas as pd
 8 | | import yaml
 9 | |
10 | | from scrapesuite.connectors.base import Connector, Raw
11 | | from scrapesuite.connectors import fda, nws
12 | | from scrapesuite.http import get_html
13 | | from scrapesuite.policy import is_allowed_domain
14 | | from scrapesuite.sinks.base import Sink
15 | | from scrapesuite.sinks.csv import CSVSink
16 | | from scrapesuite.sinks.parquet import ParquetSink
17 | | from scrapesuite.state import load_cursor, save_cursor, upsert_items
18 | | from scrapesuite.transforms.base import safe_to_iso
   | |___________________________________________________^
19 |
20 |   _CONNECTOR_REGISTRY: dict[str, type[Connector]] = {
   |
help: Organize imports

F401 [*] `os` imported but unused
 --> scrapesuite\core.py:3:8
  |
1 | """Core job loading and execution logic."""
2 |
3 | import os
  |        ^^
4 | from pathlib import Path
5 | from typing import Any
  |
help: Remove unused import: `os`

F401 [*] `pathlib.Path` imported but unused
 --> scrapesuite\core.py:4:21
  |
3 | import os
4 | from pathlib import Path
  |                     ^^^^
5 | from typing import Any
  |
help: Remove unused import: `pathlib.Path`

F401 [*] `scrapesuite.connectors.base.Raw` imported but unused
  --> scrapesuite\core.py:10:52
   |
 8 | import yaml
 9 |
10 | from scrapesuite.connectors.base import Connector, Raw
   |                                                    ^^^
11 | from scrapesuite.connectors import fda, nws
12 | from scrapesuite.http import get_html
   |
help: Remove unused import: `scrapesuite.connectors.base.Raw`

F401 [*] `scrapesuite.http.get_html` imported but unused
  --> scrapesuite\core.py:12:30
   |
10 | from scrapesuite.connectors.base import Connector, Raw
11 | from scrapesuite.connectors import fda, nws
12 | from scrapesuite.http import get_html
   |                              ^^^^^^^^
13 | from scrapesuite.policy import is_allowed_domain
14 | from scrapesuite.sinks.base import Sink
   |
help: Remove unused import: `scrapesuite.http.get_html`

F401 [*] `scrapesuite.transforms.base.safe_to_iso` imported but unused
  --> scrapesuite\core.py:18:41
   |
16 | from scrapesuite.sinks.parquet import ParquetSink
17 | from scrapesuite.state import load_cursor, save_cursor, upsert_items
18 | from scrapesuite.transforms.base import safe_to_iso
   |                                         ^^^^^^^^^^^
19 |
20 | _CONNECTOR_REGISTRY: dict[str, type[Connector]] = {
   |
help: Remove unused import: `scrapesuite.transforms.base.safe_to_iso`

UP015 [*] Unnecessary mode argument
  --> scrapesuite\core.py:34:25
   |
32 |     """Load and validate minimal YAML job spec schema."""
33 |     try:
34 |         with open(path, "r", encoding="utf-8") as f:
   |                         ^^^
35 |             data = yaml.safe_load(f)
36 |     except FileNotFoundError:
   |
help: Remove mode argument

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> scrapesuite\core.py:37:9
   |
35 |             data = yaml.safe_load(f)
36 |     except FileNotFoundError:
37 |         raise FileNotFoundError(f"Job file not found: {path}")
   |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
38 |     except yaml.YAMLError as e:
39 |         raise ValueError(f"Invalid YAML in {path}: {e}")
   |

B904 Within an `except` clause, raise exceptions with `raise ... from err` or `raise ... from None` to distinguish them from errors in exception handling
  --> scrapesuite\core.py:39:9
   |
37 |         raise FileNotFoundError(f"Job file not found: {path}")
38 |     except yaml.YAMLError as e:
39 |         raise ValueError(f"Invalid YAML in {path}: {e}")
   |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
40 |
41 |     if not isinstance(data, dict):
   |

PLR0912 Too many branches (14 > 12)
  --> scrapesuite\core.py:61:5
   |
61 | def run_job(
   |     ^^^^^^^
62 |     job_dict: dict[str, Any],
63 |     *,
   |

PLC0415 `import` should be at the top-level of a file
   --> scrapesuite\core.py:114:21
    |
112 |                 # Option C: skip if fixture missing in offline mode
113 |                 if offline:
114 |                     import warnings
    |                     ^^^^^^^^^^^^^^^
115 |
116 |                     warnings.warn(
    |

B028 No explicit `stacklevel` keyword argument found
   --> scrapesuite\core.py:116:21
    |
114 |                     import warnings
115 |
116 |                     warnings.warn(
    |                     ^^^^^^^^^^^^^
117 |                         f"Detail parser '{detail_parser_name}' specified but "
118 |                         f"fixture not found. Continuing without detail data."
    |
help: Set `stacklevel=2`

F841 Local variable `new_count` is assigned to but never used
   --> scrapesuite\core.py:138:5
    |
137 |     # Upsert to state (idempotent deduplication)
138 |     new_count = upsert_items(job_name, records, db_path=db_path)
    |     ^^^^^^^^^
139 |
140 |     # Save cursor
    |
help: Remove assignment to unused variable `new_count`

F841 Local variable `written_path` is assigned to but never used
   --> scrapesuite\core.py:155:5
    |
153 |         raise ValueError(f"Unknown sink kind '{sink_kind}'. Available: {available}")
154 |
155 |     written_path = sink.write(df, job=job_name)
    |     ^^^^^^^^^^^^
156 |
157 |     return df, next_cursor
    |
help: Remove assignment to unused variable `written_path`

I001 [*] Import block is un-sorted or un-formatted
 --> scrapesuite\http.py:3:1
  |
1 |   """Polite HTTP client with retry, backoff, and rate limiting."""
2 |
3 | / import time
4 | | import random
5 | | from typing import Optional
6 | |
7 | | import requests
  | |_______________^
8 |
9 |   _THROTTLE_LAST_TIME: Optional[float] = None
  |
help: Organize imports

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\http.py:9:22
   |
 7 | import requests
 8 |
 9 | _THROTTLE_LAST_TIME: Optional[float] = None
   |                      ^^^^^^^^^^^^^^^
10 | _RPS = 1.0
   |
help: Convert to `X | None`

PLW0603 Using the global statement to update `_THROTTLE_LAST_TIME` is discouraged
  --> scrapesuite\http.py:25:12
   |
23 |     Not used in offline mode or tests.
24 |     """
25 |     global _THROTTLE_LAST_TIME
   |            ^^^^^^^^^^^^^^^^^^^
26 |
27 |     headers = {"User-Agent": ua}
   |

F841 [*] Local variable `e` is assigned to but never used
  --> scrapesuite\http.py:43:45
   |
41 |             _THROTTLE_LAST_TIME = time.time()
42 |             return response.text
43 |         except requests.RequestException as e:
   |                                             ^
44 |             if attempt == max_retries - 1:
45 |                 raise
   |
help: Remove assignment to unused variable `e`

F401 [*] `scrapesuite.sinks.base.Sink` imported but unused
 --> scrapesuite\sinks\csv.py:7:36
  |
5 | from zoneinfo import ZoneInfo
6 |
7 | from scrapesuite.sinks.base import Sink
  |                                    ^^^^
8 | from scrapesuite.transforms.base import Frame
  |
help: Remove unused import: `scrapesuite.sinks.base.Sink`

F401 [*] `pandas` imported but unused
 --> scrapesuite\sinks\parquet.py:7:18
  |
5 | from zoneinfo import ZoneInfo
6 |
7 | import pandas as pd
  |                  ^^
8 | import pyarrow as pa
9 | import pyarrow.parquet as pq
  |
help: Remove unused import: `pandas`

F401 [*] `scrapesuite.sinks.base.Sink` imported but unused
  --> scrapesuite\sinks\parquet.py:11:36
   |
 9 | import pyarrow.parquet as pq
10 |
11 | from scrapesuite.sinks.base import Sink
   |                                    ^^^^
12 | from scrapesuite.transforms.base import Frame
   |
help: Remove unused import: `scrapesuite.sinks.base.Sink`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\transforms\base.py:10:25
   |
10 | def safe_to_iso(dt_str: Optional[str]) -> Optional[str]:
   |                         ^^^^^^^^^^^^^
11 |     """
12 |     Convert datetime string to ISO format, or return None if invalid.
   |
help: Convert to `X | None`

UP045 [*] Use `X | None` for type annotations
  --> scrapesuite\transforms\base.py:10:43
   |
10 | def safe_to_iso(dt_str: Optional[str]) -> Optional[str]:
   |                                           ^^^^^^^^^^^^^
11 |     """
12 |     Convert datetime string to ISO format, or return None if invalid.
   |
help: Convert to `X | None`

PLR2004 Magic value used in comparison, consider replacing `0.1` with a constant variable
  --> scrapesuite\wizard.py:40:16
   |
38 |     @classmethod
39 |     def validate_rps(cls, v: float) -> float:
40 |         if not 0.1 <= v <= 2.0:
   |                ^^^
41 |             raise ValueError("RPS must be between 0.1 and 2.0")
42 |         return v
   |

PLR2004 Magic value used in comparison, consider replacing `2.0` with a constant variable
  --> scrapesuite\wizard.py:40:28
   |
38 |     @classmethod
39 |     def validate_rps(cls, v: float) -> float:
40 |         if not 0.1 <= v <= 2.0:
   |                            ^^^
41 |             raise ValueError("RPS must be between 0.1 and 2.0")
42 |         return v
   |

UP032 [*] Use f-string instead of `format` call
  --> scrapesuite\wizard.py:69:30
   |
67 |     while True:
68 |         try:
69 |             response = input("Select [1-{}]: ".format(len(choices))).strip()
   |                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
70 |             if not response and default:
71 |                 return default
   |
help: Convert to f-string

PLR0912 Too many branches (16 > 12)
  --> scrapesuite\wizard.py:91:5
   |
91 | def run_wizard() -> None:
   |     ^^^^^^^^^^
92 |     """Run interactive wizard to generate job YAML."""
93 |     if console:
   |

PLR0915 Too many statements (63 > 50)
  --> scrapesuite\wizard.py:91:5
   |
91 | def run_wizard() -> None:
   |     ^^^^^^^^^^
92 |     """Run interactive wizard to generate job YAML."""
93 |     if console:
   |

PLR2004 Magic value used in comparison, consider replacing `0.1` with a constant variable
   --> scrapesuite\wizard.py:160:16
    |
158 |     try:
159 |         rps = float(rps_str)
160 |         if not 0.1 <= rps <= 2.0:
    |                ^^^
161 |             rps = 1.0
162 |     except ValueError:
    |

PLR2004 Magic value used in comparison, consider replacing `2.0` with a constant variable
   --> scrapesuite\wizard.py:160:30
    |
158 |     try:
159 |         rps = float(rps_str)
160 |         if not 0.1 <= rps <= 2.0:
    |                              ^^^
161 |             rps = 1.0
162 |     except ValueError:
    |

F401 [*] `typing.Any` imported but unused
 --> scripts\build_batches.py:5:20
  |
3 | import json
4 | from pathlib import Path
5 | from typing import Any
  |                    ^^^
6 |
7 | from scrapesuite.core import load_yaml, run_job
  |
help: Remove unused import: `typing.Any`

PLR2004 Magic value used in comparison, consider replacing `2` with a constant variable
  --> tests\test_parsers.py:51:24
   |
49 |     records, _ = connector.collect(cursor=None, max_items=10, offline=True)
50 |
51 |     if len(records) >= 2:
   |                        ^
52 |         cursor_id = records[1]["id"]
53 |         filtered_records, next_cursor = connector.collect(
   |

RUF059 Unpacked variable `next_cursor` is never used
  --> tests\test_parsers.py:53:27
   |
51 |     if len(records) >= 2:
52 |         cursor_id = records[1]["id"]
53 |         filtered_records, next_cursor = connector.collect(
   |                           ^^^^^^^^^^^
54 |             cursor=cursor_id, max_items=10, offline=True
55 |         )
   |
help: Prefix it with an underscore or any other dummy variable pattern

F401 [*] `pandas` imported but unused
 --> tests\test_run_job.py:6:18
  |
4 | from pathlib import Path
5 |
6 | import pandas as pd
  |                  ^^
7 | import pytest
  |
help: Remove unused import: `pandas`

F401 [*] `pytest` imported but unused
 --> tests\test_run_job.py:7:8
  |
6 | import pandas as pd
7 | import pytest
  |        ^^^^^^
8 |
9 | from scrapesuite.core import load_yaml, run_job
  |
help: Remove unused import: `pytest`

RUF059 Unpacked variable `cursor1` is never used
  --> tests\test_run_job.py:97:14
   |
96 |         # First run
97 |         df1, cursor1 = run_job(job_dict, max_items=10, offline=True, db_path=db_path)
   |              ^^^^^^^
98 |         assert len(df1) > 0
   |
help: Prefix it with an underscore or any other dummy variable pattern

RUF059 Unpacked variable `cursor2` is never used
   --> tests\test_run_job.py:101:14
    |
100 |         # Second run with same cursor
101 |         df2, cursor2 = run_job(job_dict, max_items=10, offline=True, db_path=db_path)
    |              ^^^^^^^
102 |
103 |         # Should have no new items (cursor filtering)
    |
help: Prefix it with an underscore or any other dummy variable pattern

F401 [*] `pytest` imported but unused
 --> tests\test_state.py:7:8
  |
5 | from pathlib import Path
6 |
7 | import pytest
  |        ^^^^^^
8 |
9 | from scrapesuite.state import load_cursor, save_cursor, upsert_items
  |
help: Remove unused import: `pytest`

PLR2004 Magic value used in comparison, consider replacing `2` with a constant variable
  --> tests\test_state.py:24:30
   |
22 |         ]
23 |         new_count1 = upsert_items("test_job", records1, db_path=db_path)
24 |         assert new_count1 == 2
   |                              ^
25 |
26 |         # Second insert with duplicates
   |

PLR2004 Magic value used in comparison, consider replacing `3` with a constant variable
  --> tests\test_state.py:41:29
   |
39 |         ).fetchall()
40 |         conn.close()
41 |         assert len(rows) == 3
   |                             ^
42 |         assert [r["id"] for r in rows] == ["001", "002", "003"]
   |

F401 [*] `scrapesuite.core.run_job` imported but unused
  --> tests\test_wizard_smoke.py:9:41
   |
 7 | import yaml
 8 |
 9 | from scrapesuite.core import load_yaml, run_job
   |                                         ^^^^^^^
10 | from scrapesuite.wizard import run_wizard
   |
help: Remove unused import: `scrapesuite.core.run_job`

PLC0415 `import` should be at the top-level of a file
  --> tests\test_wizard_smoke.py:39:21
   |
37 |                 with patch("scrapesuite.wizard._prompt_confirm", return_value=False):
38 |                     # Change to temp dir
39 |                     import os
   |                     ^^^^^^^^^
40 |
41 |                     old_cwd = os.getcwd()
   |

Found 53 errors.
[*] 32 fixable with the `--fix` option (6 hidden fixes can be enabled with the `--unsafe-fixes` option).