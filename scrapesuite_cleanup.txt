Task: Fix FDA URL parsing + make suite template-neutral + relocate example jobs

Mode: Conscientious (PHASE-1 first; ask before edits).
Constraints:
- No new dependencies.
- Keep offline tests deterministic.
- Return unified diffs only in PHASE-2, then repo tree + run steps + validation commands.

Goals
1) FDA connector: real URLs from page anchors
   - Replace synthetic URLs/IDs with real href values parsed from the listing page and urljoin(entry_url, href).
   - Derive id from the final slug segment if possible; otherwise fallback to the absolute URL.
   - Use realistic anchors in the offline fixture (tests/fixtures/fda_list.html) so offline runs produce real-looking FDA URLs.

2) Template defaults: neutral by default
   - In scrapesuite/wizard.py, change template choices to ["custom","fda_example","nws_example"] with default = "custom".
   - Update default entry and allowlist per template:
     • custom → https://example.com/, allowlist example.com
     • fda_example → FDA URL, allowlist fda.gov
     • nws_example → NWS URL, allowlist weather.gov,alerts.weather.gov
   - Generated YAML:
     • parser: custom_list (for custom), else fda_list or nws_list
     • normalize: custom (for custom), else fda_recalls or nws_alerts
     • keep optional detail_parser: fda_detail only for fda_example.

3) Relocate example jobs & limit run-all scope
   - Move jobs/fda*.yml and jobs/nws*.yml to examples/jobs/.
   - Ensure cli.py run-all scans only jobs/ (not examples/).
   - Keep examples available for reference but not executed by default.

4) Core registry import fix (if not already applied)
   - In scrapesuite/core.py, ensure normalizers import from scrapesuite.transforms, not connectors:
     NORMALIZERS = {"fda_recalls": transforms.fda.normalize, "nws_alerts": transforms.nws.normalize}

Specific edit hints (for PHASE-2 diffs)
- scrapesuite/connectors/fda.py
  • Add: from urllib.parse import urljoin; import re
  • Implement list_parser(html: str, entry_url: str) -> list[dict]:
      soup.select("a[href*='/safety/recalls-market-withdrawals-safety-alerts/']")
      url = urljoin(entry_url, a["href"])
      slug via regex r"/recalls-market-withdrawals-safety-alerts/([^/]+)$"
      rows.append({"id": slug or url, "title": a.get_text(strip=True) or slug or "Untitled", "url": url})
  • collect(...): call list_parser(list_html, entry_url=job["source"]["entry"]); remove synthetic URL/ID fabrication.

- tests/fixtures/fda_list.html
  • Ensure anchor href examples resemble real FDA paths, e.g.:
    /safety/recalls-market-withdrawals-safety-alerts/dreyers-grand-ice-cream-inc-issues-allergy-alert-undeclared-wheat-haagen-dazs-chocolate-dark

- scrapesuite/wizard.py
  • Template choices default to "custom".
  • Update defaults (_default_entry, _default_allow) and YAML rendering to map parser/normalize based on template ("custom", "fda_example", "nws_example").
  • Keep detail_parser only for fda_example.

- scrapesuite/cli.py
  • Confirm run-all only globs jobs/*.yml. No traversal into examples/.

- File moves
  • Move any FDA/NWS YAMLs from jobs/ → examples/jobs/ and adjust README examples if they reference old paths.

- scrapesuite/core.py
  • Ensure: from scrapesuite.transforms import fda as fda_tx, nws as nws_tx
  • NORMALIZERS = {"fda_recalls": fda_tx.normalize, "nws_alerts": nws_tx.normalize}

Acceptance criteria
- Offline run (FDA example) writes Parquet with realistic FDA URLs from fixture anchors (no synthetic /fda-001 patterns).
- Wizard default template is custom; generated YAML uses custom_list / normalize: custom.
- run-all ignores examples and only runs YAMLs in jobs/.
- All tests pass (pytest -q).
- Ruff clean: python -m ruff format . && python -m ruff check .

PHASE-1 (what to output first)
- Short plan listing files to touch and minimal code changes.
- Questions about fixture structure, slug regex, or YAML naming (if any).
- Any assumptions called out explicitly.

PHASE-2 (after I confirm)
- Unified diffs only for changed files and file moves.
- Then repo tree (depth=2), run steps, and validation commands.

Validation (PowerShell)
# format & lint
python -m ruff format .
python -m ruff check .

# tests
pytest -q

# regenerate a neutral job via wizard
python -m scrapesuite.cli init

# run an FDA example offline (point to examples job or create one via wizard)
python -m scrapesuite.cli run .\examples\jobsda.yml --offline true --max-items 10

# inspect latest parquet URLs
python -c "import glob,pandas as pd; p=sorted(glob.glob(r'data\cache\**\*.parquet', recursive=True))[-1]; print(p); df=pd.read_parquet(p); print(df[['id','title','url']].head(10).to_string(index=False))"

Proceed with PHASE-1 now.
