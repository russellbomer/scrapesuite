# Blind Test Guide - Interactive Wizard

Follow these steps to test the interactive wizard from scratch. No prior knowledge needed!

## Platform-Specific Setup

### Virtual Environment Activation

**Linux/Mac/WSL (bash/zsh)**:
```bash
source venv/bin/activate
```

**Windows Git Bash/MINGW64**:
```bash
source venv/Scripts/activate
```

**Windows Command Prompt (cmd.exe)**:
```cmd
venv\Scripts\activate.bat
```

**Windows PowerShell**:
```powershell
venv\Scripts\Activate.ps1
# If you get execution policy error, run first:
# Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

---

## Pre-Test Checklist

**These commands work in**: Linux, Mac, Windows Git Bash, Windows WSL

```bash
# 1. Verify you're in the right directory
pwd
# Linux/Mac: /path/to/foundry
# Windows Git Bash: /c/Projects/foundry
# Windows WSL: /mnt/c/Projects/foundry

# 2. Verify all tests pass
python -m pytest -q
# Should show: 32 passed

# 3. Verify no lint errors on critical files
python -m ruff check foundry/wizard.py foundry/inspector.py foundry/connectors/generic.py
# Should show: All checks passed! (or just warnings, not errors)
```

## Test 1: Simple Website (Hacker News)

**Goal**: Create a scraper for Hacker News stories in 5 minutes.

### Step 1: Start the wizard

```bash
python -m foundry.wizard
```

**Expected**: You see a welcome banner and template selection.

### Step 2: Select "custom" template

```
Select template
> custom          ‚Üê SELECT THIS (use arrow keys + Enter)
  fda_example
  nws_example
```

**Why**: "custom" enables the HTML analyzer for any website.

### Step 3: Enter job name

```
Job name (slug) [my_job]: hackernews_stories
```

**Type**: `hackernews_stories` and press Enter

### Step 4: Enter URL

```
Entry URL (listing page, not a single item) [https://example.com/]: 
```

**Type**: `https://news.ycombinator.com/` and press Enter

**Important**: This is the LISTING page (shows multiple stories), not a single story URL.

### Step 5: Analyze HTML

```
Analyze HTML structure and build selectors? (Y/n):
```

**Type**: `Y` or just press Enter

**Expected**: 
- You see "Analyzing HTML structure..."
- You see "‚úì Page: Hacker News"
- You see a table of detected patterns

### Step 6: Review detected patterns

**Expected output**:
```
Detected Item Patterns:
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Option ‚îÉ Selector   ‚îÉ Count ‚îÉ Sample Title             ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ 1      ‚îÇ .athing    ‚îÇ 30    ‚îÇ Some article title       ‚îÇ
‚îÇ 2      ‚îÇ .title     ‚îÇ 61    ‚îÇ                          ‚îÇ
‚îÇ 3      ‚îÇ .rank      ‚îÇ 30    ‚îÇ                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What to look for**:
- Option 1 (`.athing`) shows article titles in the "Sample Title" column
- It has 30 items (reasonable number for a page of stories)
- Other options either show no title or wrong counts

### Step 7: Select the best pattern

```
Select item pattern
> .athing (30 items)    ‚Üê SELECT THIS
  .title (61 items)
  .rank (30 items)
  Skip (use manual config)
```

**Choose**: `.athing` (option 1) because it has sample titles

### Step 8: Field selection - Title

```
Include 'title'? (preview: Ironclad ‚Äì formally verified...) (Y/n):
```

**Type**: `Y` or just press Enter

```
Selector for 'title' [span.titleline a]:
```

**Type**: Just press Enter to accept the suggestion

**What happened**: Wizard detected the title is in `span.titleline a` and showed you a preview.

### Step 9: Field selection - URL

```
Include 'url'? (preview: https://ironclad-os.org/) (Y/n):
```

**Type**: `Y` or just press Enter

```
Selector for 'url' [span.titleline a::attr(href)]:
```

**Type**: Just press Enter

**Note**: `::attr(href)` means "extract the href attribute from the link"

### Step 10: Field selection - Other fields

The wizard might ask about date, author, score, image.

**For Hacker News**:
- Date: Not visible in listing, type `N`
- Author: Not visible in listing, type `N`  
- Score: Might be detected, type `Y` if you want it
- Image: Not visible, type `N`

**Don't worry**: You only need title and URL for a basic scraper!

### Step 11: Preview extraction

**Expected**:
```
Preview of extracted data:
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ title                    ‚îÉ url                      ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ Ironclad OS              ‚îÇ https://ironclad-os.org/ ‚îÇ
‚îÇ Tabloid Language         ‚îÇ https://tabloid.vercel‚Ä¶  ‚îÇ
‚îÇ Marko Framework          ‚îÇ https://markojs.com/     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Check**:
- Do the titles look correct? (Real article titles, not navigation)
- Do the URLs look correct? (Point to actual articles, not `#` or `/new`)

### Step 12: Confirm extraction

```
Does this look correct? (Y/n):
```

**If YES**: Type `Y` or press Enter  
**If NO**: Type `N`, wizard will skip selectors and you can try again

### Step 13: Configure allowlist

```
Allowlist domains (comma-separated) [news.ycombinator.com]:
```

**Type**: Just press Enter (accepts the auto-detected domain)

**What this does**: Only allows fetching from news.ycombinator.com (safety feature)

### Step 14: Set rate limit

```
Rate limit (RPS) [1.0]:
```

**Type**: Just press Enter (1 request per second is polite)

**What this does**: Don't hammer the server with requests

### Step 15: Set cursor field

```
Cursor field [url]:
```

**Type**: Just press Enter

**What this does**: Tracks which items you've seen before to avoid duplicates

### Step 16: Choose output format

```
Sink kind
  parquet
> csv          ‚Üê SELECT CSV (easier to open)
```

**Choose**: `csv` (you can open this in Excel/Google Sheets)

### Step 17: Set output path

```
Sink path template [data/cache/%Y%m%dT%H%M%SZ.csv]:
```

**Type**: `data/cache/hn/%Y%m%dT%H%M%SZ.csv` and press Enter

**What this does**: Saves files like `data/cache/hn/20241109T143000Z.csv`

### Step 18: Set test limit

```
Max items (for smoke test) [100]:
```

**Type**: `5` and press Enter

**What this does**: Only fetch 5 items for testing (faster)

### Step 19: Run smoke test

```
Run offline smoke test? (Y/n):
```

**Type**: `N` (we want to test REAL fetching, not cached data)

**Expected**:
```
SUCCESS: Job spec written to jobs/hackernews_stories.yml
```

### Step 20: Verify the config was created

```bash
ls jobs/hackernews_stories.yml
cat jobs/hackernews_stories.yml
```

**Expected**: File exists and contains:
```yaml
version: "1"
job: hackernews_stories
source:
  kind: html
  entry: https://news.ycombinator.com/
  parser: generic
  ...
selectors:
  item: ".athing"
  fields:
    title: "span.titleline a"
    url: "span.titleline a::attr(href)"
```

### Step 21: Test the scraper FOR REAL

```bash
python -c "
from foundry.core import load_yaml, run_job
spec = load_yaml('jobs/hackernews_stories.yml')
df, cursor = run_job(spec, max_items=5, offline=False)
print(f'\n‚úì Collected {len(df)} items\n')
print(df[['title', 'url']].to_string())
"
```

**Expected output**:
```
‚úì Collected 5 items

                                   title                              url
0  Ironclad ‚Äì formally verified OS  https://ironclad-os.org/
1  Tabloid: The Clickbait Language  https://tabloid.vercel.app/
2  Marko ‚Äì HTML‚Äëbased framework     https://markojs.com/
3  Some other article              https://example.com/article
4  Another story                   https://news.example.com/story
```

**Success criteria**:
- ‚úì You got 5 items (or fewer if HN has fewer stories)
- ‚úì Titles are real article titles (not "1.", "2.", "Hacker News")
- ‚úì URLs are real article URLs (not vote links, not empty)

---

## Test 2: Different Website (Product Hunt)

**Goal**: Prove the wizard works on a DIFFERENT site.

### Quick Test

```bash
python -m foundry.wizard
```

**Settings**:
1. Template: `custom`
2. Job name: `producthunt_test`
3. URL: `https://www.producthunt.com/` 
4. Analyze HTML: `Y`
5. Select pattern: Look for one with product names in "Sample Title"
6. Include fields: title, url (say No to others for speed)
7. Confirm preview: `Y` if it looks right
8. Rate limit: `0.5` (be extra polite to PH)
9. Output: `csv`
10. Path: `data/cache/ph/%Y%m%dT%H%M%SZ.csv`
11. Max items: `3`
12. Smoke test: `N`

**Then test**:
```bash
python -c "
from foundry.core import load_yaml, run_job
spec = load_yaml('jobs/producthunt_test.yml')
df, cursor = run_job(spec, max_items=3, offline=False)
print(f'\n‚úì Collected {len(df)} items\n')
print(df[['title', 'url']].to_string())
"
```

**Expected**: 3 product names with URLs

---

## Test 3: Robots.txt Check

**Goal**: Verify robots.txt checking works.

### Test on a site that BLOCKS scraping

```bash
python -c "
from foundry.policy import check_robots
url = 'https://www.reddit.com/'
allowed = check_robots(url)
print(f'Reddit scraping allowed: {allowed}')
"
```

**Expected**: `False` (Reddit blocks scrapers)

### Test on a site that ALLOWS scraping

```bash
python -c "
from foundry.policy import check_robots
url = 'https://github.com/'
allowed = check_robots(url)
print(f'GitHub scraping allowed: {allowed}')
"
```

**Expected**: `True` (GitHub allows with crawl-delay)

---

## Test 4: Error Handling

**Goal**: Verify wizard handles bad input gracefully.

### Test 1: Bad URL

```bash
python -m foundry.wizard
```

**Enter**:
- Template: `custom`
- Job name: `error_test`
- URL: `not-a-valid-url`
- Analyze: `Y`

**Expected**: Error message about invalid URL, wizard stops gracefully

### Test 2: No patterns found

Some sites with heavy JavaScript won't work:

```bash
python -m foundry.wizard
```

**Enter**:
- Template: `custom`
- URL: `https://twitter.com/` (might not work - JS-heavy)
- Analyze: `Y`

**Expected**: "No repeated patterns found. Using simple link extraction." message

---

## Test 5: Existing Tests

**Goal**: Ensure nothing broke.

```bash
# Run all 32 tests
python -m pytest -v

# Should see:
# tests/test_generic_connector.py::test_generic_connector_basic PASSED
# tests/test_generic_connector.py::test_generic_connector_attribute_extraction PASSED
# tests/test_generic_connector.py::test_generic_connector_missing_selectors PASSED
# ... (29 more tests)
# ======================== 32 passed ========================
```

---

## Troubleshooting Guide

### Problem: Windows Unicode encoding error

**Symptom**:
```
UnicodeEncodeError: 'charmap' codec can't encode character '\u2011'
```

**Fix**: Already handled in the code! Pull the latest version:
```bash
git pull origin chore/context-freeze
```

The test scripts now automatically detect Windows and use UTF-8 encoding.

### Problem: "ImportError: cannot import name..."

**Fix**:
```bash
# Reinstall dependencies
pip install -r requirements.txt
```

### Problem: "No module named foundry"

**Fix**:
```bash
# Make sure you're in the right directory
cd /workspaces/foundry
pwd  # Should show /workspaces/foundry
```

### Problem: Wizard extracts wrong data

**Check**:
1. Did you choose the right pattern? (Look at "Sample Title" column)
2. Is the preview correct? (If not, say "N" and try different pattern)
3. Is the site JavaScript-heavy? (View source - if you see mostly `<div id="root">`, it won't work)

### Problem: "Offline mode not supported"

**This is expected** - GenericConnector requires live fetching. Just use `offline=False`:

```python
df, cursor = run_job(spec, max_items=5, offline=False)
```

### Problem: Empty results (0 items)

**Debug**:
```bash
# Check if the site blocks you
python -c "
from foundry.policy import check_robots
print(check_robots('https://YOURSITE.com/'))
"

# Try fetching manually
python -c "
from foundry.http import get_html
html = get_html('https://YOURSITE.com/')
print(f'Fetched {len(html)} bytes')
print(html[:500])  # First 500 chars
"
```

---

## Success Criteria Summary

After running all tests, you should have:

‚úÖ **Test 1**: `jobs/hackernews_stories.yml` that successfully extracts 5 HN stories  
‚úÖ **Test 2**: `jobs/producthunt_test.yml` that successfully extracts 3 products  
‚úÖ **Test 3**: robots.txt checks return correct True/False  
‚úÖ **Test 4**: Wizard handles errors without crashing  
‚úÖ **Test 5**: All 32 tests passing  

**If all 5 pass**: The wizard is production-ready! üéâ

---

## Next Steps After Testing

1. **Read `WIZARD_USAGE.md`** for detailed docs
2. **Try more sites**: Medium, GitHub trending, dev.to
3. **Check `TROUBLESHOOTING.md`** for diagnostic commands
4. **Share your configs**: Save working YAMLs in `jobs/` for reuse

---

## Quick Reference Card

### For Linux/Mac/Git Bash/WSL

```bash
# Start wizard
python -m foundry.wizard

# Test a job
python -c "from foundry.core import load_yaml, run_job; spec = load_yaml('jobs/JOBNAME.yml'); df, _ = run_job(spec, max_items=5, offline=False); print(df)"

# Check robots.txt
python -c "from foundry.policy import check_robots; print(check_robots('https://example.com/'))"

# Run all tests
python -m pytest -q

# Validate a job config
cat jobs/JOBNAME.yml
```

### For Windows Command Prompt / PowerShell

If `python -c "..."` doesn't work, create a file `test_job.py`:

```python
# test_job.py
from foundry.core import load_yaml, run_job

spec = load_yaml('jobs/JOBNAME.yml')
df, _ = run_job(spec, max_items=5, offline=False)
print(df)
```

Then run: `python test_job.py`

---

**YOU GOT THIS!** üí™ Follow the steps, read the output, and the wizard will guide you.
