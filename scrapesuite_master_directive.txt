ScrapeSuite — Master Directive for Cursor (Conscientious Mode)

Intent
------
This document is the single source of truth for building and hardening ScrapeSuite.
Cursor must operate in Conscientious Mode: STOP & ASK whenever anything is ambiguous, and complete work in small, verifiable steps. Each stage begins with PHASE 1 (Plan & Questions) and proceeds to PHASE 2 (Apply Changes) only after I confirm.

Global Rules
------------
- Do NOT install new dependencies.
- Keep CLI and job schema backward compatible unless explicitly authorized.
- All tests must be offline/deterministic (no network).
- Respect “Option B” for connector config (see below).
- Return UNIFIED DIFFS ONLY when applying changes, then a repo tree (depth=2), and PowerShell validation commands.
- Be token-efficient in chat replies (no emojis, no fluff). Code quality is NOT constrained by token limits.

Architecture Rule (Q1)
----------------------
Connector config passing: Choose Option B.
- Every connector stores `entry_url` (and config) in `__init__`.
- Keep `collect(self, cursor: Optional[str], max_items: int, offline: bool = True)` unchanged.
- Core factory ALWAYS passes `entry_url` explicitly from YAML.
- FDA/NWS examples may keep a temporary default with DeprecationWarning if `entry_url=None`. Wizard-generated connectors MUST require `entry_url` (no hidden default).

Custom Connector Presence (Q2)
------------------------------
Ship stubs so the default “custom” template works out-of-the-box offline.
- Registries: map "custom_list" → scrapesuite.connectors.custom:CustomConnector and "custom" → scrapesuite.transforms.custom:normalize.
- CustomConnector: Option B `__init__`, `collect()` returns 3 deterministic records offline; in live mode raises NotImplementedError with a friendly message.
- Custom normalizer: returns DataFrame with columns ["id","source","title","url","posted_at"]; `source="custom"`; ISO date coercion.
- Fixture: tests/fixtures/custom_list.html with 3 realistic anchors.
- Wizard default template uses parser: custom_list and normalize: custom.

Fixture URLs (Q3)
-----------------
Use synthetic-but-realistic FDA-style slugs (deterministic). Include 2 relative and 1 absolute href within FDA path structure. Purpose: exercise urljoin + slug extraction; no live links in tests.

Delivery Plan (Stages)
----------------------
Execute stages in order. Each stage starts with PHASE 1 (Plan & Questions) and proceeds to PHASE 2 (Apply) only after confirmation.

Stage A — Cleanup & Baseline (must pass CI)
A1) FDA connector fixes
    - Implement Option B `__init__(entry_url, allowlist=None, rate_limit_rps=1.0)`.
    - Parse real anchors in list_parser(html, entry_url) using selector:
      a[href*='/safety/recalls-market-withdrawals-safety-alerts/']
      url = urljoin(entry_url, href)
      id  = last slug via regex r"/recalls-market-withdrawals-safety-alerts/([^/]+)$" (fallback to absolute URL).
    - Update collect() to use stored entry_url; remove synthetic URL/ID fabrication.
    - Fixtures: tests/fixtures/fda_list.html with synthetic realistic URLs (2 relative, 1 absolute).

A2) NWS connector parity
    - Add Option B `__init__(entry_url, allowlist=None, rate_limit_rps=1.0)` and route collect() via instance config (parsing logic may remain as-is).

A3) Core factory and registries
    - Instantiate connectors with explicit entry_url, allowlist, rate_limit_rps.
    - Ensure transform registry imports point to scrapesuite.transforms.* (not connectors) for normalizers.

A4) Wizard defaults & YAML generation
    - Template choices: ["custom", "fda_example", "nws_example"]; default = "custom".
    - Defaults mapping:
      custom       → entry: https://example.com/ ; allowlist: ["example.com"] ; parser: custom_list ; normalize: custom
      fda_example  → current FDA values ; include detail_parser only for FDA
      nws_example  → current NWS values
    - Guardrail: if entry URL looks like a detail page (extra slug after listing segment), warn and re-prompt.
    - Ensure allowlist auto-includes urlparse(entry).netloc.

A5) Example jobs relocation
    - Move jobs/fda*.yml and jobs/nws*.yml to examples/jobs/ .
    - Verify run-all only globs jobs/*.yml (non-recursive).

A6) Custom stubs
    - scrapesuite/connectors/custom.py (Option B): offline returns 3 deterministic records; live raises NotImplementedError.
    - scrapesuite/transforms/custom.py: normalize() to standard columns.
    - tests/fixtures/custom_list.html with 3 anchors and titles.

A7) Tests
    - test_parsers.py
      • FDA: absolute URL via urljoin; id = slug; next_cursor == first id.
      • Custom: ≥3 rows; required keys; stable next_cursor.
      • Deprecation path: FDA/NWS raises DeprecationWarning if entry_url=None (pytest.warns).
    - test_run_job.py
      • Custom job offline writes a batch with expected columns.
      • Ensure run-all ignores examples/jobs.
    - No network in tests; offline only.

A8) CI green
    - ruff format/check and pytest must pass.

Stage B — Generic Selector Connector (fallback path)
B1) Connector & normalizer
    - connectors/generic_selector.py → GenericSelectorConnector(entry_url, allowlist, rate_limit_rps, selector_cfg, id_strategy).
    - transforms/generic.py → normalize(records) with standard columns.
    - Registry entries: "generic_selector_list" and "generic".
B2) YAML shape supported
    source.selector.item, href_attr, title (":self" allowed), posted_at ("time::attr(datetime)" style), id_strategy {type: last_path_segment|url_regex|query_param|text_selector, ...}.
B3) Wizard branch (fallback)
    - Menu: “Use generic selector connector (YAML only)” appears AFTER the default generator.
    - Wizard collects YAML fields, writes jobs/<slug>.yml, optional fixtures, offers offline run.
B4) Tests
    - Selector permutations (at least 2), id strategies (at least 2), posted_at parsing, and error messages for missing selectors.

Stage C — Wizard Connector Generator (default path)
C1) Guided codegen
    - Wizard branch “Create new connector (guided)” (DEFAULT path).
    - Generate: connectors/<slug>.py (Option B), transforms/<slug>.py, jobs/<slug>.yml, tests/fixtures/<slug>_list.html (+opt detail), tests in test_parsers.py and test_run_job.py.
C2) Prompts
    - Name (slug), Entry URL (listing) with validation, Allowlist (default includes entry netloc), Item/anchor selector, href attr, title selector or :self, posted_at selector (optional), ID strategy (last_path_segment|url_regex|query_param|text_selector), Detail enrichment (y/N + field map), Sink kind/path, RPS, max_items.
C3) Behavior
    - After generation: offer an offline smoke test.
    - If validation fails: STOP & ASK, do not proceed.

Stage D — Demo command (one-command success)
D1) CLI `scrapesuite demo`
    - Copy a demo job (custom_demo.yml) into ./jobs/ if missing.
    - Run it offline and report written file paths.
D2) Bundle fixtures in package (importlib.resources) so demo works after `pip install`.

Validation (PowerShell)
-----------------------
Run after each stage applies:
  python -m ruff format .
  python -m ruff check .
  pytest -q

Stage A quick smoke:
  python -m scrapesuite.cli init     # choose custom
  python -m scrapesuite.cli run .\jobs\custom_*.yml --offline true --max-items 10
  python -c "import glob,pandas as pd; p=sorted(glob.glob(r'data\cache\**\*.parquet', recursive=True))[-1]; print(p); df=pd.read_parquet(p); print(df[['id','title','url']].head(5).to_string(index=False))"

Proceed Sequence
----------------
1) Start with STAGE A — PHASE 1 (Plan & Questions). List files to touch, exact edits, tests to add, and ask clarifying questions (if any). Do NOT modify files yet.
2) After confirmation, STAGE A — PHASE 2 (Apply Changes): return UNIFIED DIFFS ONLY, then repo tree (depth=2), and the validation commands above.
3) When Stage A is green, repeat the same process for STAGE B, then STAGE C, then STAGE D.

Assumptions
-----------
- Python 3.11+.
- pandas/pyarrow/bs4 already available.
- No network in tests; offline fixtures only.
- FDA/NWS default URLs may emit DeprecationWarning until jobs are regenerated via wizard.
