You are building a reusable Python toolkit for web/data collection named **ScrapeSuite** (internal name only in code). The objective is a production-quality, testable library + CLI that runs fully offline in CI using fixtures, and optionally runs “live” in developer environments with respectful policies.

===============================================================================
CONSCIENTIOUS MODE (MANDATORY)
===============================================================================
Priority: **Caution, communication, and user confirmation over speed.**
Operate in a strict two-phase loop for every non-trivial action:

PHASE 1 — PLAN & QUESTIONS
- Produce:
  1) A concise plan of intended changes (files to touch, functions to add, tests to write).
  2) Specific, numbered clarifying questions for anything remotely ambiguous.
  3) Any assumptions you propose to make, each prefixed with “ASSUMPTION:” and a short rationale.
- Then **STOP** and request confirmation. Do **not** create/modify files yet.

PHASE 2 — APPLY (only after explicit user confirmation)
- Implement exactly what was confirmed.
- If uncertainty arises during implementation, **pause and return to PHASE 1**.
- After applying, present **unified diffs only**, then repo tree + run steps + validation commands.

If any directive conflicts with Conscientious Mode, **Conscientious Mode wins**. Modify the directive and notify the user.

===============================================================================
TOKEN EFFICIENCY FOR CHAT (DOES NOT APPLY TO CODE)
===============================================================================
- Applies only to natural-language chat replies, not to code, diffs, tests, or documentation files.
- Keep replies terse and functional: avoid emojis, stylistic filler, and marketing tone.
- Prefer bullets and numbered lists over paragraphs when asking questions or summarizing plans.
- Default max length guideline: ~150–250 words unless user asks for more.
- Never reduce code quality, test coverage, or documentation clarity to save tokens.

===============================================================================
SCOPE
===============================================================================
- Python library + CLI to:
  - Define scraping “jobs” in YAML.
  - Run connectors (HTML; requests + BeautifulSoup) with retry/backoff and rate limits.
  - Transform raw records into normalized tables (pandas).
  - Write outputs to sinks (Parquet/CSV).
  - Maintain idempotent state (SQLite): per-job cursor and (job,id) dedupe.
  - Enforce basic “politeness” policy (allowlist, robots stub, RPS profiles).
  - Provide a CLI **wizard** to generate a job YAML and run an offline smoke test.
- All tests run **offline** using local HTML fixtures (no network).
- Provide a batch builder script that executes jobs and emits timestamped Parquet chunks + `index.json`.
- Ship with CI (ruff + pytest), Makefile, and a clear README.

===============================================================================
CONSTRAINTS (HARD)
===============================================================================
- Python: >= 3.11 (compatible with 3.11/3.12/3.13).
- Dependencies ONLY:
    requests
    beautifulsoup4
    pandas
    pyarrow
    pyyaml
    pytest
    typer
    questionary
    pydantic
    rich
- Style: ruff (format + lint; line-length=100). Type-hint everything (pyright-strict friendly).
- Tests + default “init/smoke” runs must be **offline** and deterministic with fixtures.
- Provide polite HTTP utilities, but they are **not used** in tests (fixtures only).

===============================================================================
OUTPUT FLOW (RESPECT CONSCIENTIOUS MODE)
===============================================================================
- First response in any new task cycle: **PHASE 1** (Plan + Questions + Assumptions). **Do not** output diffs yet.
- After user confirms: **PHASE 2** output:
  - **UNIFIED DIFFS ONLY** for created/changed files.
  - Then:
    1) repo tree (depth=2)
    2) “How to run locally”
    3) three validation commands

===============================================================================
REPO LAYOUT (CREATE ALL)
===============================================================================
pyproject.toml
requirements.txt
Makefile
.github/workflows/ci.yml
scrapesuite/
  __init__.py
  cli.py                # Typer CLI (init | run | run-all | state)
  core.py               # job loader, run loop, offline/live switch
  state.py              # sqlite helpers (jobs_state + items upsert)
  policy.py             # allowlist, robots stub, rate profiles
  http.py               # polite HTTP (UA, 1rps, retry/backoff) – not used in tests
  wizard.py             # interactive “init” wizard (Typer+Questionary+Pydantic+Rich)
  connectors/
    base.py
    fda.py              # example HTML connector
    nws.py              # example HTML connector
  transforms/
    base.py
    fda.py              # normalize()
    nws.py              # normalize()
  sinks/
    base.py
    parquet.py
    csv.py
jobs/
  fda.yml               # example job spec
  nws.yml               # example job spec
scripts/
  build_batches.py      # runs all jobs → timestamped parquet + index.json
tests/
  fixtures/
    fda_list.html
    fda_detail.html
    nws_list.html
  test_state.py
  test_parsers.py
  test_run_job.py
  test_wizard_smoke.py
data/cache/.gitkeep
README.md

===============================================================================
PROJECT CONFIG
===============================================================================
pyproject.toml
- [project] name="scrapesuite", description, requires-python=">=3.11"
- [tool.ruff]
  line-length = 100
  select = ["E","F","I","UP","B","PL","RUF"]
  ignore = []
- [tool.ruff.format]
  quote-style = "preserve"

requirements.txt
- Exactly these deps (no pins unless requested):
  requests
  beautifulsoup4
  pandas
  pyarrow
  pyyaml
  pytest
  typer
  questionary
  pydantic
  rich

Makefile
- fmt: ruff format .
- lint: ruff check .
- test: pytest -q
- init: python -m scrapesuite.cli init
- run-fda: python -m scrapesuite.cli run jobs/fda.yml --max-items 100 --offline true
- run-nws: python -m scrapesuite.cli run jobs/nws.yml --max-items 100 --offline true
- build-batches: python scripts/build_batches.py

.github/workflows/ci.yml
- On push & PR
- Matrix python-version: [3.11, 3.12, 3.13]
- Steps: checkout, setup-python, pip install -r requirements.txt, ruff format --check ., ruff check ., pytest -q

README.md (top)
- What the suite is (library + CLI).
- Offline-first examples with fixtures.
- Quickstart (install, init wizard, run a job offline).
- Policy note: live runs must respect allowlist/robots and rate limit.

===============================================================================
JOB SPEC SCHEMA (YAML)
===============================================================================
Example `jobs/fda.yml`
version: "1"
job: fda_recalls
source:
  kind: html
  entry: https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts
  parser: fda_list          # scrapesuite.connectors.fda:list_parser()
  detail_parser: fda_detail # optional per-item detail fetch/parse
  rate_limit_rps: 1
  cursor:
    field: id
    stop_when_seen: true
transform:
  pipeline:
    - normalize: fda_recalls # scrapesuite.transforms.fda:normalize()
sink:
  kind: parquet
  path: data/cache/fda/%Y%m%dT%H%M%SZ.parquet
policy:
  robots: allow
  allowlist: ["fda.gov"]

Example `jobs/nws.yml`
- Similar with `parser: nws_list`, `normalize: nws_alerts`, allowlist: ["weather.gov","alerts.weather.gov"]

Schema rules:
- `version` is string "1".
- `job` is a slug (letters, numbers, underscores).
- `source.kind` ∈ {"html"} (future extensible to "api").
- `source.parser` maps to a callable in the appropriate connectors module.
- `source.detail_parser` optional string for detail enrichment.
- `source.cursor.field` name used to mark “seen” items (e.g., id).
- `transform.pipeline` is an ordered list of steps; each `normalize: <name>` maps to a function in transforms.
- `sink.kind` ∈ {"parquet","csv"}; `sink.path` supports `strftime` tokens (UTC).
- `policy.allowlist` must be honored for live runs; in tests/offline this is not invoked.

===============================================================================
IMPLEMENTATION DETAILS
===============================================================================

scrapesuite/__init__.py
- Expose version string and minimal public API (e.g., run_job).

scrapesuite/cli.py  (Typer)
- Commands:
  - `init` → runs the interactive wizard (wizard.run_wizard()).
  - `run <job_yaml> [--max-items N] [--offline true|false]` → executes one job; writes via sink; upserts items; updates cursor; prints summary `{job}: {new} new, {len(df)} in batch, next_cursor=...`.
  - `run-all [--max-items N] [--offline true|false]` → executes all YAML in /jobs in alpha order.
  - `state [--db PATH]` → prints per-job last_cursor and last_run.
- Errors: return non-zero; provide helpful, actionable messages.

scrapesuite/wizard.py (Typer + Questionary + Pydantic + Rich)
- `WizardModel(BaseModel)`:
  job_name (slug), template (Literal["fda","nws","custom"]),
  entry (HttpUrl), allowlist (List[str]), rps (float 0.1..2.0),
  cursor_field (str), sink_kind (Literal["parquet","csv"]),
  sink_path (str), max_items (int > 0)
- `run_wizard()`:
  - Prompts with sensible defaults per template (FDA/NWS).
  - Validates with Pydantic; pretty errors via Rich.
  - Writes `jobs/<job_name>.yml` using the schema above.
  - Offers to run offline smoke test (fixtures) by calling `core.run_job(..., offline=True)`; prints results.
- If Questionary/Rich not available at runtime, gracefully fall back to stdlib input/print.

scrapesuite/core.py
- `load_yaml(path: str) -> dict` (validate minimal schema; helpful errors).
- `run_job(job_dict: dict, *, max_items: int = 200, offline: bool = True) -> tuple[pandas.DataFrame, str | None]`
  - Resolve connector by `source.parser` (and optional detail parser).
  - If `offline=True`: read from `tests/fixtures/*.html` (deterministic). If `offline=False`: use http.get_html.
  - Apply transform pipeline (currently just normalize).
  - Return `(df, next_cursor)` where `next_cursor` is typically the first seen id.
- Enforce policy:
  - If `offline=False`, ensure all outbound URLs are in `policy.allowlist`; else error.
  - Throttle/backoff from `http.py` used only in live mode.

scrapesuite/state.py
- SQLite at `data/cache/state.sqlite` by default (create dirs as needed).
- Tables:
  - `jobs_state(job TEXT PRIMARY KEY, last_cursor TEXT, last_run TEXT)`
  - `items(job TEXT, id TEXT, payload_json TEXT, first_seen TEXT, last_seen TEXT, PRIMARY KEY(job,id))`
- Functions:
  - `open_db(path: str | None = None) -> sqlite3.Connection` (row_factory=sqlite3.Row)
  - `load_cursor(job: str) -> str | None`
  - `save_cursor(job: str, cursor: str | None) -> None`
  - `upsert_items(job: str, records: list[dict]) -> int` (idempotent insert/update by (job,id); returns count of newly inserted rows)

scrapesuite/policy.py
- `is_allowed_domain(url: str, allowlist: list[str]) -> bool`
- `check_robots(url: str) -> bool`  # stub: return True for now with TODO
- `RateProfile` dataclass: `rps: float` (default 1.0), `burst: int` (default 1)

scrapesuite/http.py
- `get_html(url: str, *, ua: str = "ScrapeSuite/1.0 (+contact@example.com)", timeout=15) -> str`
  - requests.get with UA; retry (3) and exponential backoff (0.5, 1, 2 s)
  - process-global 1 rps throttle (time.sleep with jitter)
  - Not used in tests/offline runs.

scrapesuite/connectors/base.py
- TypedDict `Raw`: id:str; url:str; title:str; posted_at:str; body:str (all optional via total=False)
- Protocol `Connector` with:
  - `collect(self, cursor: Optional[str], max_items: int, offline: bool = True) -> tuple[list[Raw], Optional[str]]`

scrapesuite/connectors/fda.py
- `FDAConnector(Connector)`:
  - `collect(cursor, max_items, offline)`:
    - If offline: read `tests/fixtures/fda_list.html` and `tests/fixtures/fda_detail.html`; synthesize 3–5 records; deterministic order; include stable `id`.
    - If live: fetch list page via http.get_html; optionally fetch detail pages (respect allowlist).
  - `list_parser(html: str) -> list[dict]`, `detail_parser(html: str) -> dict`
  - Fields per record: id (recall number), title, url, posted_at (string), class ("I"/"II"/"III"|None), category ("Food"/"Drug"/"Device"|None), brand (str|""), reason (str|""), description (short).
  - `next_cursor = first record id` if present; else None.

scrapesuite/connectors/nws.py
- `NWSConnector(Connector)`:
  - Offline: read `tests/fixtures/nws_list.html`.
  - Fields: id, type ("Warning"|"Watch"|"Advisory"), area (text), severity (str|None), start (string), end (string|None), headline (title), url.
  - `next_cursor = first id`.

scrapesuite/transforms/base.py
- Alias `Frame = pandas.DataFrame`
- `safe_to_iso(dt_str: str | None) -> str | None` using `pandas.to_datetime(..., utc=True, errors="coerce")` → `.isoformat()` or None.

scrapesuite/transforms/fda.py
- `normalize(records: list[dict]) -> Frame`
  - Columns: [id, source="fda", title, url, posted_at(ISO str), class, class_weight:int (I=3, II=2, III=1 else 1), category, brand, reason, description]

scrapesuite/transforms/nws.py
- `normalize(records: list[dict]) -> Frame`
  - Columns: [id, source="nws", title=headline, url, posted_at=start(ISO str), type, area, severity, severity_weight:int (Warning=3, Watch=2, Advisory=1 default 1), end(ISO|None)]

scrapesuite/sinks/base.py
- Protocol `Sink`: `write(self, df: Frame) -> str`

scrapesuite/sinks/parquet.py
- `ParquetSink(path_template: str)`
- `write(df) -> str`: expand utcnow with `strftime`, ensure parent dirs, write Parquet (pyarrow), return file path.

scrapesuite/sinks/csv.py
- `CSVSink(path_template: str)`
- `write(df) -> str`: expand path, write CSV, return file path.

scripts/build_batches.py
- Iterate all YAML files in `/jobs`.
- For each:
  - Load last cursor from `state`.
  - Run with `offline=True` by default (fixtures).
  - Write Parquet to `/data/cache/<job>/<UTC-timestamp>.parquet`.
  - Append entry to `/data/cache/index.json` (JSON object keyed by job → list of batch filenames in order).

tests/fixtures/*.html
- Provide tiny representative HTML for FDA list & detail and NWS list so parsers can extract 3–5 items. Keep fixtures minimal and stable.

tests/test_state.py
- Temp DB; `upsert_items` with duplicate ids increments count only on first insert; cursor save/load works.

tests/test_parsers.py
- Load fixtures; run connectors in offline mode; assert record keys present and non-empty; `next_cursor == first id`.

tests/test_run_job.py
- Load `jobs/fda.yml` and `jobs/nws.yml`; `run_job(..., offline=True)` returns a DataFrame with expected columns; use a temporary sink path override; state cursor advances between two runs (second run yields 0 new inserts if same fixtures).

tests/test_wizard_smoke.py
- Mock inputs for `wizard.run_wizard()`; ensure YAML is written; load YAML; call `run_job(..., offline=True)`; assert rows > 0.

README.md
- Overview, installation, quickstart:
  1) `pip install -r requirements.txt`
  2) `make init` → follow prompts; choose FDA or NWS; generate YAML
  3) `python -m scrapesuite.cli run jobs/<name>.yml --offline true`
- Notes on policy and live mode (allowlist + robots + RPS).

===============================================================================
STOP & ASK CHECKLIST (use during every PHASE 1)
===============================================================================
Before implementing, ask if any are unclear:
1) Naming: package name/import path, CLI command name, job slugs.
2) Schema details: required vs optional fields in YAML; cursor semantics.
3) Fixture fidelity: minimal HTML shape, record counts, deterministic IDs.
4) Policy defaults: default RPS, allowlist behavior in live mode, robots stub stance.
5) State location: default SQLite path; whether to allow override via env/flag.
6) Error handling: desired UX for user errors vs internal errors.
7) Wizard UX: preferred defaults and validation strictness.
8) Test thresholds: minimum rows expected from fixtures; ordering guarantees.

===============================================================================
INITIAL RUN — PHASE 1 STARTER PLAN (USE THIS INITIALLY)
===============================================================================
Plan (no file writes yet):
- Create scaffolding plan for repo layout (files only as a proposal).
- Propose minimal YAML schema (keys, required/optional).
- Propose exact CLI commands and flags.
- Propose fixture structure and record shape for FDA/NWS (3–5 items).
- Propose DB schema for state (jobs_state, items).
- Propose ruff config and CI matrix (3.11–3.13).
- Propose wizard question flow and defaults.

Clarifying questions:
1) Package/import name: confirm `scrapesuite` and CLI command `scrapesuite`?
2) License preference and README tone (concise/technical vs tutorial)?
3) Default SQLite path: `data/cache/state.sqlite` acceptable? Allow `--db PATH` override?
4) YAML schema: is `transform.pipeline` limited to just one `normalize` step for v1?
5) Wizard defaults: pick FDA as default template? Default RPS=1.0? Max items default=100?
6) Fixture size: OK with ~3–5 items per source for deterministic tests?
7) Error policy: fail fast on unknown parser/sink names with actionable message?
8) Output path templates: OK with UTC `%Y%m%dT%H%M%SZ` naming?
9) Dev Python versions: target 3.11/3.12/3.13 in CI—any others?
10) Any naming/style preferences for code (e.g., snake_case only, no abbreviations)?

Assumptions (pending your confirmation):
- ASSUMPTION: Keep dependencies limited to the listed set; no extra libs (e.g., Playwright).
- ASSUMPTION: Live HTTP is off by default; tests run offline only.
- ASSUMPTION: Allowlist is mandatory in live mode; robots check is a stub for now.
- ASSUMPTION: Minimal logging with concise one-line summaries.

Upon confirmation, proceed to PHASE 2: generate initial scaffolding with unified diffs, then tree + run steps + validation commands.
